{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Concatenate, Conv1D, Reshape, Flatten, MaxPool1D, GlobalMaxPool1D, GlobalAvgPool1D, Multiply, Dot, Input, Attention, BatchNormalization, Dropout, Lambda, Input, Dense, Concatenate\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "#tf.keras.backend.clear_session()\n",
    "# tf.config.experimental_run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NanumGothic']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# 폰트 설정 방법 1\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib 폰트설정\n",
    "# plt.rc('font', family='NanumGothicOTF') # For MacOS\n",
    "plt.rc('font', family='NanumGothic') # For Windows\n",
    "print(plt.rcParams['font.family'])\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "ans = pd.read_csv(\"answer_sheet.csv\")\n",
    "stocks = pd.read_csv('stocks.csv')\n",
    "trade = pd.read_csv('trade_train.csv')\n",
    "\n",
    "num2name = dict((stocks[['종목번호','종목명']].drop_duplicates()).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1087/1087 [07:55<00:00,  2.29it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le2 = LabelEncoder()\n",
    "\n",
    "col = '종목번호'\n",
    "le.fit(stocks[col].append(trade[col]))\n",
    "stocks[col] = le.transform(stocks[col].fillna('NaN'))\n",
    "#stocks[col] = stocks[col].astype('category')\n",
    "trade[col] = le.transform(trade[col].fillna('NaN'))\n",
    "#trade[col] = trade[col].astype('category')\n",
    "            \n",
    "col = '그룹번호'\n",
    "le2.fit(trade[col])\n",
    "trade[col] = le2.transform(trade[col].fillna('NaN'))\n",
    "#trade[col] = trade[col].astype('category')\n",
    "\n",
    "# 전고점, 전저점 변수 생성\n",
    "from tqdm import tqdm\n",
    "stocks['전고점돌파'] = np.nan\n",
    "stocks['전저점돌파'] = np.nan\n",
    "for i in tqdm(sorted(set(stocks['종목번호']))):\n",
    "    global_max = 0\n",
    "    global_min = 999999999999\n",
    "    for j in sorted(set(stocks.loc[stocks['종목번호']==i, '기준일자'])):\n",
    "        idx = (stocks['종목번호']==i) & (stocks['기준일자']==j)\n",
    "        if stocks.loc[idx, '종목고가'].values[0] >= global_max:\n",
    "            global_max = stocks.loc[idx, '종목고가'].values[0]\n",
    "            stocks.loc[idx, '전고점돌파'] = 1\n",
    "        if stocks.loc[idx, '종목저가'].values[0] <= global_min:\n",
    "            global_min = stocks.loc[idx, '종목저가'].values[0]\n",
    "            stocks.loc[idx, '전저점돌파'] = 1\n",
    "\n",
    "stocks[['전고점돌파','전저점돌파']] = stocks[['전고점돌파','전저점돌파']].fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "# trade 파생변수\n",
    "trade['매수고객수_비율'] = trade['매수고객수']/trade['그룹내고객수']\n",
    "trade['매도고객수_비율'] = trade['매도고객수']/trade['그룹내고객수']\n",
    "trade['평균매수단가'] = trade['평균매수수량']*trade['매수가격_중앙값']*trade['매수고객수']\n",
    "trade['평균매도단가'] = trade['평균매도수량']*trade['매도가격_중앙값']*trade['매도고객수']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks['기준년월'] = stocks['기준일자']//100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_low_sum = stocks.groupby(['종목번호','기준년월'])[['전고점돌파','전저점돌파']].sum().reset_index()\n",
    "stocks_max = stocks.loc[(stocks.groupby([\"종목번호\",\"기준년월\"])[\"거래량\"].idxmax()).dropna()]\n",
    "stocks_max = pd.merge(stocks_max, high_low_sum, how='left', on=['종목번호','기준년월'], suffixes=['', '_합계'])\n",
    "\n",
    "stocks_max['기준년월'] = stocks_max['기준년월'] - 1\n",
    "stocks_max['기준년월'] = stocks_max['기준년월'].replace(202000, 201912)\n",
    "\n",
    "trade = pd.merge(trade, stocks_max, how='left', on=['종목번호','기준년월'])\n",
    "trade = trade.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13912/13912 [03:27<00:00, 67.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# 주가 스케일링\n",
    "from tqdm import tqdm\n",
    "target = ['종목시가','종목고가','종목저가','종목종가', '거래량','거래금액_만원단위']\n",
    "for i, j in tqdm(stocks[['기준년월','종목번호']].drop_duplicates().values):\n",
    "    stocks.loc[(stocks['기준년월']==i) & (stocks['종목번호']==j), target] =  stocks.loc[(stocks['기준년월']==i) & (stocks['종목번호']==j), target]/stocks.loc[(stocks['기준년월']==i) & (stocks['종목번호']==j), target].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:04<00:00,  2.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# top3 추가, \n",
    "# 월별 그룹별 매수, 매도 고객수 스케일링\n",
    "#trade = trade.fillna(0)\n",
    "from tqdm import tqdm\n",
    "#for i in sorted(list(trade['그룹번호'])):\n",
    "trade['top3'] = 0\n",
    "top = 3\n",
    "#i = sorted(list(trade['그룹번호']))[0]\n",
    "for m in tqdm(sorted(set(trade['기준년월']))):\n",
    "    for i in sorted(set(trade['그룹번호'])):\n",
    "        idx = trade.loc[(trade['그룹번호']==i) & (trade['기준년월']==m),'매수고객수'].sort_values().index[-top:]\n",
    "        trade.loc[idx,'top3'] = 1\n",
    "        trade.loc[(trade['그룹번호']==i) & (trade['기준년월']==m),'매수고객수']/=trade.loc[(trade['그룹번호']==i) & (trade['기준년월']==m),'매수고객수'].max()\n",
    "        trade.loc[(trade['그룹번호']==i) & (trade['기준년월']==m),'매도고객수']/=trade.loc[(trade['그룹번호']==i) & (trade['기준년월']==m),'매도고객수'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp = trade.copy()\n",
    "tmp['기준년월'] = tmp['기준년월'] + 1\n",
    "tmp['기준년월'] = tmp['기준년월'].replace(201913, 202001)\n",
    "trade = pd.merge(trade, tmp[['매수고객수','기준년월','종목번호','그룹번호']], how='left', on=['기준년월','종목번호','그룹번호'], \n",
    "        suffixes=['_정답', '_전월'])\n",
    "trade = pd.merge(trade, tmp[['기준년월','종목번호','그룹번호','매수가격_중앙값','매도가격_중앙값']], how='left', on=['기준년월','종목번호','그룹번호'], \n",
    "        suffixes=['', '_전월'])\n",
    "\n",
    "trade = trade.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = trade.copy()\n",
    "tmp['기준년월'] = tmp['기준년월'] + 1\n",
    "tmp['기준년월'] = tmp['기준년월'].replace(201913, 202001)\n",
    "trade = pd.merge(trade, tmp[['매수고객수_전월','기준년월','종목번호','그룹번호']], how='left', on=['기준년월','종목번호','그룹번호'], \n",
    "        suffixes=['', '_2전월'])\n",
    "\n",
    "trade = trade.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = '매수가격_중앙값'\n",
    "trade[col] = trade[col].replace(-1, 0)\n",
    "trade['매수가격_종목시가_비율'] = trade[col]/trade['종목시가']\n",
    "trade['매수가격_종목고가_비율'] = trade[col]/trade['종목고가']\n",
    "trade['매수가격_종목저가_비율'] = trade[col]/trade['종목저가']\n",
    "trade['매수가격_종목종가_비율'] = trade[col]/trade['종목종가']\n",
    "\n",
    "col = '매도가격_중앙값'\n",
    "trade[col] = trade[col].replace(-1, 0)\n",
    "trade['매도가격_종목시가_비율'] = trade[col]/trade['종목시가']\n",
    "trade['매도가격_종목고가_비율'] = trade[col]/trade['종목고가']\n",
    "trade['매도가격_종목저가_비율'] = trade[col]/trade['종목저가']\n",
    "trade['매도가격_종목종가_비율'] = trade[col]/trade['종목종가']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade = trade.replace([np.inf, -np.inf], np.nan).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2536/2536 [00:30<00:00, 83.96it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "month = 202003\n",
    "#month = 201902\n",
    "stock_info = []\n",
    "trade_info = []\n",
    "y_list = []\n",
    "y_list_top = []\n",
    "\n",
    "for i, j in tqdm(trade[trade['기준년월']>=month][['기준년월','종목번호']].drop_duplicates().values):\n",
    "#         stock_info.append(stocks.loc[(stocks['기준년월']==i) & (stocks['종목명']==j), ['종목시가','종목고가','종목저가','종목종가','거래량'\n",
    "#                                                               ,'거래금액_만원단위','전고점돌파','전저점돌파']].values)\n",
    "        \n",
    "        # if j not in set(stocks.loc[stocks['20년7월TOP3대상여부']=='Y','종목번호']):\n",
    "        #     continue\n",
    "\n",
    "    stock_seq = stocks.loc[(stocks['기준년월']==i) & (stocks['종목번호']==j), ['종목시가','종목고가','종목저가','종목종가','거래량'\n",
    "                                                        ,'거래금액_만원단위','전고점돌파','전저점돌파']].values\n",
    "    if len(stock_seq) <19 :\n",
    "        continue\n",
    "    elif len(stock_seq) == 19 :\n",
    "        stock_seq = np.vstack((stock_seq, np.zeros((1,stock_seq.shape[1]))))\n",
    "    \n",
    "    # trade_seq = trade.loc[(trade['기준년월']==i) & (trade['종목번호']==j), ['그룹번호','매수고객수_전월','매수고객수_전월_2전월','top3']].values\n",
    "    # y_list.append(trade.loc[(trade['기준년월']==i) & (trade['종목번호']==j), '매수고객수_정답'].values)\n",
    "\n",
    "    trade_seq = trade.loc[(trade['기준년월']==i) & (trade['종목번호']==j), ['기준년월','종목번호','그룹번호','매수고객수_전월', '매수고객수_정답','top3']]\n",
    "    trade_seq2 = trade.loc[(trade['기준년월']==i-1) & (trade['종목번호']==j), ['기준년월','종목번호','그룹번호'] + \n",
    "                                    ['전고점돌파_합계', '전저점돌파_합계','매수가격_종목시가_비율',\n",
    "'매수가격_종목고가_비율', '매수가격_종목저가_비율', '매수가격_종목종가_비율', '매도가격_종목시가_비율',\n",
    "'매도가격_종목고가_비율', '매도가격_종목저가_비율', '매도가격_종목종가_비율']].copy()\n",
    "                                \n",
    "    trade_seq2['기준년월'] += 1\n",
    "\n",
    "    trade_seq = pd.merge(trade_seq, trade_seq2, on=['기준년월','종목번호','그룹번호'], how='left')\n",
    "\n",
    "    y_list.append(trade_seq.loc[(trade_seq['기준년월']==i) & (trade_seq['종목번호']==j), '매수고객수_정답'].values)\n",
    "    y_list_top.append(trade_seq.loc[(trade_seq['기준년월']==i) & (trade_seq['종목번호']==j), 'top3'].values)\n",
    "\n",
    "    trade_seq = trade_seq.drop(['기준년월','종목번호','매수고객수_정답','top3'], axis=1).fillna(0).values\n",
    "\n",
    "\n",
    "    trade_info.append(trade_seq)\n",
    "    for _ in range(len(trade_seq)):\n",
    "        stock_info.append(stock_seq[:20])\n",
    "            \n",
    "            #y_list.append(trade.loc[(trade['기준년월']==i) & (trade['종목번호']==j), 'top3'].values)\n",
    "\n",
    "            \n",
    "stock_info = np.array(stock_info)\n",
    "trade_info = np.vstack(trade_info)\n",
    "y_list = (np.hstack(y_list)).reshape(-1,1)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_list_top = np.hstack(y_list_top).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_list_top = np.hstack((y_list_top==0, y_list_top==1)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_list_top = np.hstack((y_list_top==0, y_list_top==1)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:00<00:00, 221.67it/s]\n"
     ]
    }
   ],
   "source": [
    "test_stock_info = []\n",
    "test_trade_info = []\n",
    "item_num = []\n",
    "i = 202007\n",
    "for j in tqdm(sorted(set(stocks.loc[stocks['20년7월TOP3대상여부']=='Y','종목번호']))):\n",
    "    stock_seq = stocks.loc[(stocks['기준년월']==i) & (stocks['종목번호']==j), ['종목번호','종목시가','종목고가','종목저가','종목종가','거래량'\n",
    "                                                            ,'거래금액_만원단위','전고점돌파','전저점돌파']].values\n",
    "    if len(stock_seq) <19 :\n",
    "            continue\n",
    "    if len(stock_seq) == 19 :\n",
    "        stock_seq = np.vstack((stock_seq, np.zeros((1,stock_seq.shape[1]))))\n",
    "    \n",
    "    # trade_seq = trade.loc[(trade['기준년월']==i-1) & (trade['종목번호']==j), ['그룹번호','매수고객수_정답','매수고객수_전월']].values\n",
    "    # trade_seq = np.hstack((trade_seq, np.ones((len(trade_seq),1))))\n",
    "    trade_seq = trade.loc[(trade['기준년월']==i-1) & (trade['종목번호']==j), ['그룹번호','매수고객수_정답'] +\n",
    "    ['전고점돌파_합계', '전저점돌파_합계','매수가격_종목시가_비율',\n",
    "                   '매수가격_종목고가_비율', '매수가격_종목저가_비율', '매수가격_종목종가_비율', '매도가격_종목시가_비율',\n",
    "                          '매도가격_종목고가_비율', '매도가격_종목저가_비율', '매도가격_종목종가_비율']].copy().values\n",
    "                                    \n",
    "        \n",
    "                             \n",
    "    test_trade_info.append(trade_seq)\n",
    "    for _ in range(len(trade_seq)):\n",
    "        test_stock_info.append(stock_seq[:20])\n",
    "            \n",
    "test_stock_info = np.array(test_stock_info)\n",
    "test_trade_info = np.vstack(test_trade_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Concatenate, Conv1D, Reshape, Flatten, MaxPool1D, GlobalMaxPool1D, GlobalAvgPool1D, Multiply, Dot, Input, Attention, BatchNormalization, Dropout, Lambda, Input, Dense, Concatenate\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "\n",
    "\n",
    "# tf.config.experimental_run_functions_eagerly(True)\n",
    "\n",
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(\n",
    "            query, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(\n",
    "            key, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(\n",
    "            value, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        return output\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    \n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "    # def get_config(self):\n",
    "    #     config = super().get_config().copy()\n",
    "    #     config.update({\n",
    "    #         'embed_dim': self.embed_dim,\n",
    "    #         'num_heads': self.num_heads,\n",
    "    #         'ff_dim': self.ff_dim\n",
    "    #     })\n",
    "    #     return config\n",
    "\n",
    "class PositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, embed_dim):\n",
    "        super(PositionEmbedding, self).__init__()\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def call(self, x):\n",
    "        positions = tf.range(start=0, limit=self.maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        return x + positions\n",
    "    \n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 20, 8)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 20, 64)       576         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "position_embedding (PositionEmb (None, 20, 64)       1280        conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "transformer_block (TransformerB (None, 20, 64)       49984       position_embedding[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "transformer_block_1 (Transforme (None, 20, 64)       49984       transformer_block[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 64)           0           transformer_block_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 12)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 8)            520         global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 128)          1664        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 136)          0           dense_12[0][0]                   \n",
      "                                                                 dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 136)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 128)          17536       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 128)          0           dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 64)           8256        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 1)            65          dense_15[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 129,865\n",
      "Trainable params: 129,865\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import random as rn\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "seed_num = 77\n",
    "np.random.seed(seed_num)\n",
    "rn.seed(seed_num)\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "from tensorflow.keras import backend as K\n",
    "tf.random.set_seed(seed_num)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "\n",
    "# 200706 Transformer Model 2\n",
    "num_layers = 2\n",
    "num_heads = 8  # Number of attention heads\n",
    "embed_dim = 64\n",
    "#embed_dim = \n",
    "ff_dim = 4*embed_dim  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "num_features = 8\n",
    "time_step = 20\n",
    "\n",
    "\n",
    "trade_inputs = layers.Input(shape = (trade_info.shape[1],))\n",
    "stock_inputs = layers.Input(shape = (time_step, num_features))\n",
    "\n",
    "x = layers.Conv1D(filters = embed_dim, kernel_size=1, padding='same')(stock_inputs)\n",
    "#x = layers.Reshape((1,embed_dim))(dense_total)\n",
    "#embed_dim = num_features\n",
    "pos_encoding_layer = PositionEmbedding(time_step, embed_dim)\n",
    "x = pos_encoding_layer(x)\n",
    "for _ in range(num_layers):\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "    x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dense(8, activation=\"relu\")(x)\n",
    "x2 = layers.Dense(128, activation='relu')(trade_inputs)\n",
    "x = layers.Concatenate()([x, x2])\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "outputs = layers.Dense(1)(x)\n",
    "model = Model([stock_inputs, trade_inputs], [outputs])\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "52/52 [==============================] - 4s 72ms/step - loss: 0.1515 - val_loss: 0.0168\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.0523 - val_loss: 0.0179\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 1s 14ms/step - loss: 0.0411 - val_loss: 0.0163\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 1s 14ms/step - loss: 0.0365 - val_loss: 0.0183\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.0339 - val_loss: 0.0162\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.0314 - val_loss: 0.0174\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 1s 14ms/step - loss: 0.0302 - val_loss: 0.0158\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.0279 - val_loss: 0.0142\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.0264 - val_loss: 0.0130\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.0250 - val_loss: 0.0120\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.0231 - val_loss: 0.0110\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 1s 14ms/step - loss: 0.0213 - val_loss: 0.0107\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.0206 - val_loss: 0.0113\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.0195 - val_loss: 0.0099\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.0183 - val_loss: 0.0098\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.0177 - val_loss: 0.0098\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.0172 - val_loss: 0.0087\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.0166 - val_loss: 0.0086\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.0163 - val_loss: 0.0085\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.0155 - val_loss: 0.0084\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.0156 - val_loss: 0.0084\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.0156 - val_loss: 0.0087\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.0159 - val_loss: 0.0085\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.0154 - val_loss: 0.0088\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.0150 - val_loss: 0.0083\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.0149 - val_loss: 0.0083\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.0146 - val_loss: 0.0084\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.0147 - val_loss: 0.0093\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.0145 - val_loss: 0.0081\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.0148 - val_loss: 0.0083\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.0146 - val_loss: 0.0079\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 1s 16ms/step - loss: 0.0142 - val_loss: 0.0079\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.0140 - val_loss: 0.0085\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 1s 14ms/step - loss: 0.0142 - val_loss: 0.0082\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.0140 - val_loss: 0.0079\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 1s 13ms/step - loss: 0.0141 - val_loss: 0.0087\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.0136 - val_loss: 0.0079\n",
      "Epoch 38/100\n",
      "47/52 [==========================>...] - ETA: 0s - loss: 0.0139\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 5.0000002374872565e-05.\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.0139 - val_loss: 0.0085\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.0139 - val_loss: 0.0078\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0134 - val_loss: 0.0079\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0134 - val_loss: 0.0079\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.0136 - val_loss: 0.0078\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0133 - val_loss: 0.0080\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0136 - val_loss: 0.0080\n",
      "Epoch 45/100\n",
      "51/52 [============================>.] - ETA: 0s - loss: 0.0134\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 5.000000237487257e-06.\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0134 - val_loss: 0.0078\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0133 - val_loss: 0.0079\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.0136 - val_loss: 0.0079\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0136 - val_loss: 0.0078\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.0135 - val_loss: 0.0078\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.0136 - val_loss: 0.0078\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.0136 - val_loss: 0.0079\n",
      "Epoch 52/100\n",
      "51/52 [============================>.] - ETA: 0s - loss: 0.0136\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 5.000000328436726e-07.\n",
      "52/52 [==============================] - 1s 13ms/step - loss: 0.0136 - val_loss: 0.0079\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f16ec0d8790>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.0005)\n",
    "model.compile(optimizer=optimizer, loss=['mse'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, min_delta=1e-4, mode='min')\n",
    "model.fit(x=[stock_info, trade_info], y=y_list, batch_size=256, epochs=100, \n",
    "          validation_split=0.2,\n",
    "          callbacks=[early_stop, reduce_lr_loss],\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict([test_stock_info[:,:,1:], test_trade_info])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame({'종목번호':np.array(test_stock_info)[:,0,0], '그룹번호':np.vstack(test_trade_info)[:,0], '예측':y_pred.flatten()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred['종목번호'] = le.inverse_transform(df_pred['종목번호'].astype(int))\n",
    "df_pred['그룹번호'] = le2.inverse_transform(df_pred['그룹번호'].astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "for i in sorted(set(df_pred['그룹번호'])):\n",
    "    y_pred_list.append(sorted(df_pred[df_pred['그룹번호']==i][['종목번호','예측']].sort_values(by='예측', ascending=False)['종목번호'][:3].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMRegressor\n",
    "from tsfresh import extract_features\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade = pd.read_csv('trade_train.csv', index_col=0)\n",
    "stock = pd.read_csv('stocks.csv', index_col=0)\n",
    "answer = pd.read_csv('answer_sheet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade = trade.merge(stock.loc[:, ['종목번호', '시장구분', '표준산업구분코드_대분류', '표준산업구분코드_중분류', '표준산업구분코드_소분류']].drop_duplicates(), on='종목번호')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade['y'] = trade['매수고객수'] / trade['그룹내고객수']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr2num = trade[['그룹번호', '그룹내고객수']].drop_duplicates().set_index('그룹번호').to_dict()['그룹내고객수']\n",
    "trade_dict = trade.set_index(['기준년월', '그룹번호', '종목번호']).to_dict()\n",
    "j_list = list(trade['기준년월'].unique()) + [202007]\n",
    "k_list = trade['그룹번호'].unique()\n",
    "for i in stock[stock['20년7월TOP3대상여부'] == 'Y']['종목번호'].unique():\n",
    "    for j in j_list:\n",
    "        for k in k_list:\n",
    "            if (j, k, i) not in trade_dict['그룹내고객수']:\n",
    "                trade_dict['y'][(j, k, i)] = 0\n",
    "                trade_dict['그룹내고객수'][(j, k, i)] = gr2num[k]\n",
    "                trade_dict['그룹내_매수여부'][(j, k, i)] = 'N'\n",
    "                trade_dict['그룹내_매도여부'][(j, k, i)] = 'N'\n",
    "                trade_dict['매수고객수'][(j, k, i)] = 0\n",
    "                trade_dict['매도고객수'][(j, k, i)] = 0\n",
    "                trade_dict['평균매수수량'][(j, k, i)] = -1\n",
    "                trade_dict['평균매도수량'][(j, k, i)] = -1\n",
    "                trade_dict['매수가격_중앙값'][(j, k, i)] = -1\n",
    "                trade_dict['매도가격_중앙값'][(j, k, i)] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade = pd.DataFrame(trade_dict).reset_index()\n",
    "trade = trade.rename({'level_0':'기준년월', 'level_1':'그룹번호', 'level_2':'종목번호'}, axis=1)\n",
    "trade = trade.sort_values(['그룹번호', '기준년월', '종목번호'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade = trade[trade['종목번호'].isin(stock[stock['20년7월TOP3대상여부'] == 'Y']['종목번호'].unique())].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = stock[stock['20년7월TOP3대상여부'] == 'Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade['전월_매수고객수'] = None\n",
    "trade['평균_매수고객수'] = None\n",
    "trade['전월_매도고객수'] = None\n",
    "trade['전월_평균매수수량'] = None\n",
    "trade['전월_평균매도수량'] = None\n",
    "trade['전월_매수가격_중앙값'] = None\n",
    "trade['전월_매도가격_중앙값'] = None\n",
    "trade_dict = trade.set_index(['기준년월', '그룹번호', '종목번호']).to_dict()\n",
    "trade_idx_dict = trade.to_dict()\n",
    "buys = {}\n",
    "for i in trade.index:\n",
    "    pre_date = trade_idx_dict['기준년월'][i] - 1 if trade_idx_dict['기준년월'][i] != 202001 else 201912\n",
    "    key = (pre_date, trade_idx_dict['그룹번호'][i], trade_idx_dict['종목번호'][i])\n",
    "    if key in trade_dict['매수고객수']:\n",
    "        trade_idx_dict['전월_매수고객수'][i] = trade_dict['매수고객수'][key]\n",
    "        if key[1:] in buys:\n",
    "            trade_idx_dict['평균_매수고객수'][i] = max(buys[key[1:]])\n",
    "            buys[key[1:]].append(trade_dict['매수고객수'][key])\n",
    "        else:\n",
    "            buys[key[1:]] = [trade_dict['매수고객수'][key]]\n",
    "        trade_idx_dict['전월_매도고객수'][i] = trade_dict['매도고객수'][key]\n",
    "        trade_idx_dict['전월_평균매수수량'][i] = trade_dict['평균매수수량'][key]\n",
    "        trade_idx_dict['전월_평균매도수량'][i] = trade_dict['평균매도수량'][key]\n",
    "        trade_idx_dict['전월_매수가격_중앙값'][i] = trade_dict['매수가격_중앙값'][key]\n",
    "        trade_idx_dict['전월_매도가격_중앙값'][i] = trade_dict['매도가격_중앙값'][key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade = pd.DataFrame(trade_idx_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = stock.sort_values(['종목번호', '기준일자'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_month = 201906\n",
    "price = 0\n",
    "stock['종가스케일'] = None\n",
    "stock_dict = stock.to_dict()\n",
    "for i in stock.index:\n",
    "    now_month = stock_dict['기준일자'][i] // 100\n",
    "    if now_month != pre_month:\n",
    "        pre_month = now_month\n",
    "        price = stock_dict['종목종가'][i]\n",
    "    stock_dict['종가스케일'][i] = stock_dict['종목종가'][i] / price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = pd.DataFrame(stock_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in stock[(stock['종목시가'] == 0)].index:\n",
    "    stock.loc[i, '종목시가'] = stock.loc[i, '종목종가']\n",
    "    stock.loc[i, '종목고가'] = stock.loc[i, '종목종가']\n",
    "    stock.loc[i, '종목저가'] = stock.loc[i, '종목종가']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = stock.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock['기준년월'] = stock['기준일자'].apply(lambda x : x // 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock['7일변화량'] = None\n",
    "stock['3일변화량'] = None\n",
    "stock['연속상승'] = 0\n",
    "stock['상승여부'] = 0\n",
    "pre_month = 201906\n",
    "stock_dict = stock.set_index(['종목번호', '기준일자']).to_dict()\n",
    "stock_idx_dict = stock.to_dict()\n",
    "for i in stock.index:\n",
    "    if stock_idx_dict['기준년월'] != pre_month:\n",
    "        stock_idx_dict['연속상승'][i] = 0\n",
    "        stock_idx_dict['상승여부'][i] = 0\n",
    "        pre_month = stock_idx_dict['기준년월']\n",
    "    else:\n",
    "        if stock_idx_dict['종목종가'][i] > stock_idx_dict['종목종가'][i-1]:\n",
    "            stock_idx_dict['연속상승'][i] = stock_idx_dict['연속상승'][i-1] + 1\n",
    "            stock_idx_dict['상승여부'][i] = 1\n",
    "        \n",
    "    if (stock_idx_dict['종목번호'][i], stock_idx_dict['기준일자'][i] - 7) in stock_dict['종목종가']:\n",
    "        stock_idx_dict['7일변화량'][i] = stock_idx_dict['종목종가'][i] / stock_dict['종목종가'][(stock_idx_dict['종목번호'][i], stock_idx_dict['기준일자'][i] - 7)]\n",
    "    if (stock_idx_dict['종목번호'][i], stock_idx_dict['기준일자'][i] - 3) in stock_dict['종목종가']:\n",
    "        stock_idx_dict['3일변화량'][i] = stock_idx_dict['종목종가'][i] / stock_dict['종목종가'][(stock_idx_dict['종목번호'][i], stock_idx_dict['기준일자'][i] - 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = pd.DataFrame(stock_idx_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = stock[stock['종목번호'].isin(set(trade['종목번호']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = stock.sort_values(['종목번호', '기준일자'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock['id'] = stock['기준년월'].astype('str') + '_' + stock['종목번호']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock['가격_변화량'] = (stock['종목종가'] - stock['종목시가']) / stock['종목시가']\n",
    "stock['고가_변화량'] = (stock['종목고가'] - stock['종목시가']) / stock['종목시가']\n",
    "stock['저가_변화량'] = (stock['종목저가'] - stock['종목시가']) / stock['종목시가']\n",
    "stock['저가_고가_변화량'] = (stock['종목고가'] - stock['종목저가']) / stock['종목저가']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock['가격_변화비율'] = 1 + stock['가격_변화량']\n",
    "stock['고가_변화비율'] = 1 + stock['고가_변화량']\n",
    "stock['저가_변화비율'] = 1 + stock['저가_변화량']\n",
    "stock['저가_고가_변화비율'] = 1 + stock['저가_고가_변화량']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade['id'] = trade['기준년월'].astype('str') + '_' + trade['종목번호']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84240/84240 [00:10<00:00, 7890.51it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in range(23):\n",
    "    trade['가격{}'.format(i)] = None\n",
    "    trade['전월가격{}'.format(i)] = None\n",
    "    \n",
    "trade_dict = trade.to_dict()\n",
    "stock_dict = stock.set_index(['종목번호', '기준일자']).to_dict()\n",
    "\n",
    "for i in tqdm(trade.index):\n",
    "    j = 0\n",
    "    for day in range(1, 32):\n",
    "        day_tmp = trade_dict['기준년월'][i] * 100 + day\n",
    "        try:\n",
    "            trade_dict['가격{}'.format(j)][i] = stock_dict['종가스케일'][(trade_dict['종목번호'][i], day_tmp)]\n",
    "            j += 1\n",
    "        except:\n",
    "            pass\n",
    "    pre_month = trade.loc[i, '기준년월'] - 1 if trade.loc[i, '기준년월'] != 202001 else 201912\n",
    "    j = 0\n",
    "    for day in range(1, 32):\n",
    "        day_tmp = pre_month * 100 + day\n",
    "        try:\n",
    "            trade_dict['전월가격{}'.format(j)][i] = stock_dict['종가스케일'][(trade_dict['종목번호'][i], day_tmp)]\n",
    "            j += 1\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "trade = pd.DataFrame(trade_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries = stock[stock['id'].isin(set(trade['id']))][['id', '기준일자', '상승여부']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries = timeseries.fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 115/115 [00:05<00:00, 19.86it/s]\n"
     ]
    }
   ],
   "source": [
    "features_filtered_direct = extract_features(timeseries, column_id='id', column_sort='기준일자')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_filtered_direct = features_filtered_direct.reset_index()\n",
    "features_filtered_direct = features_filtered_direct.rename({'index':'id'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade = trade.merge(features_filtered_direct, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in trade.columns:\n",
    "    if not (trade[c].dtype == 'float64' or trade[c].dtype == 'int'):\n",
    "        trade[c] = trade[c].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade['pre_y'] = trade['전월_매수고객수'] / trade['그룹내고객수']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in trade.columns:\n",
    "    if not (trade[c].dtype == 'float64' or trade[c].dtype == 'int'):\n",
    "        trade[c] = trade[c].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-59-c8da248f7752>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['가격0'] = X['가격0'].astype('float')\n",
      "<ipython-input-59-c8da248f7752>:10: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  X_ = X[(trade['기준년월'] > 201907) & (trade['기준년월'] != 202007)]\n",
      "<ipython-input-59-c8da248f7752>:11: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  X__ = X[(trade['기준년월']== 202007)]\n",
      "<ipython-input-59-c8da248f7752>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trade['예측'][(trade['기준년월']== 202007)] = lg.predict(X__)\n"
     ]
    }
   ],
   "source": [
    "trade['예측'] = None\n",
    "\n",
    "price = ['가격{}'.format(i) for i in range(18)]\n",
    "pre_price = ['전월가격{}'.format(i) for i in range(18)]\n",
    "X = trade[['pre_y', '상승여부__sum_values'] + price + pre_price]\n",
    "X['가격0'] = X['가격0'].astype('float')\n",
    "X = X.dropna()\n",
    "\n",
    "\n",
    "X_ = X[(trade['기준년월'] > 201907) & (trade['기준년월'] != 202007)]\n",
    "X__ = X[(trade['기준년월']== 202007)]\n",
    "lg = LGBMRegressor(num_leaves=7, reg_lambda=0.1)\n",
    "lg.fit(X_, trade.loc[X_.index, 'y'])\n",
    "trade['예측'][(trade['기준년월']== 202007)] = lg.predict(X__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in answer.index:\n",
    "    tmp = trade[['종목번호', '예측']][(trade['기준년월'] == 202007) & (trade['그룹번호'] == answer.loc[i, '그룹명'])].sort_values('예측', ascending=False)\n",
    "    tmp2 = sorted([tmp.iloc[0, 0], tmp.iloc[1, 0], tmp.iloc[2, 0]])\n",
    "    answer.loc[i, '종목번호1'] = tmp2[0]\n",
    "    answer.loc[i, '종목번호2'] = tmp2[1]\n",
    "    answer.loc[i, '종목번호3'] = tmp2[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df = trade[['그룹번호', '종목번호', '예측']][(trade['기준년월']== 202007)].sort_values(['그룹번호', '예측', '종목번호'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = json_df\n",
    "df2 = df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df1, df2, how='inner', on=['그룹번호','종목번호'])\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df['예측_x'] = scaler.fit_transform(df['예측_x'].values.reshape(-1,1))\n",
    "df['예측_y'] = scaler.fit_transform(df['예측_y'].values.reshape(-1,1))\n",
    "\n",
    "w = 0.65\n",
    "df['예측'] = w*df['예측_x'] + (1-w)*df['예측_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "for i in sorted(set(df['그룹번호'])):\n",
    "    y_pred_list.append(sorted(df[df['그룹번호']==i][['종목번호','예측']].sort_values(by='예측', ascending=False)['종목번호'][:3].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = pd.read_csv(\"answer_sheet.csv\")\n",
    "ans.iloc[:,1:] = y_pred_list\n",
    "ans.to_csv('submit.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
